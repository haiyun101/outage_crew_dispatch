import pandas as pd
import numpy as np
import time
from stable_baselines3 import PPO
from src.eval_env import CrewDispatchEvalEnv
from src.data_loader import load_and_process_data
from src.config import *
from collections import defaultdict

# --- Define Baselines ---

class RandomAgent:
    """Baseline 1: Randomly moves crews around."""
    def __init__(self, action_space):
        self.action_space = action_space
        
    def predict(self, obs, deterministic=True):
        return self.action_space.sample(), None

class GreedyAgent:
    """
    Baseline 2: The 'Do Nothing' / Reactive Agent.
    
    It outputs Action=0 (Stay) for all crews.
    Because the environment has built-in logic to 'auto-dispatch to nearest outage'
    when a crew is idle, Action=0 effectively means "Stay at station until needed, 
    then go to nearest job."
    """
    def __init__(self, num_crews):
        self.num_crews = num_crews
        
    def predict(self, obs, deterministic=True):
        # Return [0, 0, 0, ...] -> All crews stay/wait for assignments
        return [0] * self.num_crews, None

# --- Evaluation Loop ---

def run_evaluation(agent_name, agent, env, n_episodes=3):
    """Runs the agent in the environment and tracks CMO."""
    print(f"ğŸ§ Evaluating {agent_name}...")
    total_cmos = []
    
    for i in range(n_episodes):
        obs, _ = env.reset()
        done = False
        episode_cmo = 0.0
        
        while not done:
            action, _ = agent.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # Extract CMO from info dict (preferred) or infer from reward
            # Since reward = -CMO in our EvalEnv:
            step_cmo = -reward 
            episode_cmo += step_cmo
            
        total_cmos.append(episode_cmo)
        print(f"   > Episode {i+1}: Total CMO = {episode_cmo:,.0f}")
        
    avg_cmo = np.mean(total_cmos)
    print(f"   ğŸ‘‰ Average CMO: {avg_cmo:,.0f}\n")
    return avg_cmo

def main():
    # ==========================================
    # 1. å…³é”®ä¿®å¤ï¼šæ„å»ºâ€œå…¨é‡åœ°å›¾â€ (Master Map)
    # ==========================================
    print("ğŸ—ºï¸ Building Master Town Map from Full Training Data (to match model shape)...")
    # æˆ‘ä»¬åŠ è½½è®­ç»ƒæœŸçš„æ•°æ®ï¼Œä»…ä»…æ˜¯ä¸ºäº†æ‹¿åˆ°æ‰€æœ‰åŸé•‡çš„åˆ—è¡¨å’Œåæ ‡
    # è¿™æ ·èƒ½ä¿è¯ Evaluation ç¯å¢ƒçš„ Observation Space å’Œ Training æ—¶ä¸€æ¨¡ä¸€æ · (1440,)
    train_outages, _, train_town_names = load_and_process_data(
        OUTAGE_FILE, DENSITY_FILE, FORECAST_FILE, TRAIN_START_DATE, TRAIN_END_DATE
    )
    
    coords_df = train_outages.drop_duplicates(subset="City/Town")[["City/Town", "Longitude", "Latitude"]]
    master_towns_map = {row["City/Town"]: (row["Longitude"], row["Latitude"]) 
                        for _, row in coords_df.iterrows() if row["City/Town"] in train_town_names}
    
    print(f"   âœ… Master Map loaded with {len(master_towns_map)} towns.")

    # ==========================================
    # 2. åŠ è½½â€œåœ£è¯é£æš´â€è¯„ä¼°æ•°æ®
    # ==========================================
    print("\nğŸ„ Loading Christmas Storm Data (2022-12-22 to 2022-12-26)...")
    eval_outages, eval_forecast, _ = load_and_process_data(
        OUTAGE_FILE, DENSITY_FILE, FORECAST_FILE, "2022-12-22", "2022-12-26"
    )
    
    # æ„å»ºå¤©æ°”æŸ¥è¯¢å­—å…¸
    forecast_dict = defaultdict(lambda: defaultdict(dict))
    if not eval_forecast.empty:
        for _, row in eval_forecast.iterrows():
            forecast_dict[row["datetime"]][row["town"]][row["forecast_hour"]] = row["predicted_prob"]

    # ==========================================
    # 3. åˆå§‹åŒ–è¯„ä¼°ç¯å¢ƒ (ä½¿ç”¨ Master Map!)
    # ==========================================
    env = CrewDispatchEvalEnv(
        towns=master_towns_map,  # ğŸ‘ˆ å…³é”®ç‚¹ï¼šä¼ å…¥ 90 ä¸ªåŸé•‡çš„å…¨é‡åœ°å›¾ï¼Œè€Œä¸æ˜¯ 35 ä¸ª
        stations=STATIONS,
        outages_df=eval_outages,
        forecast_df=eval_forecast,
        crew_per_station=CREWS_PER_STATION,
        forecast_dict=forecast_dict
    )
    
    num_crews = len(env.action_space.nvec)
    print(f"   âœ… Environment initialized. Action Space size: {num_crews} crews.")

    # ==========================================
    # 4. åŠ è½½é€‰æ‰‹ (Agents)
    # ==========================================
    agents = {}
    
    # A. Random
    agents["Random"] = RandomAgent(env.action_space)
    
    # B. Greedy (The benchmark)
    agents["Greedy (Standard)"] = GreedyAgent(num_crews)
    
    # C. Your PPO Model
    print("\nğŸ¤– Loading Trained PPO Model...")
    try:
        # åŠ è½½æ¨¡å‹
        ppo_model = PPO.load("ppo_crew_dispatch_final.zip")
        agents["PPO AI"] = ppo_model
    except FileNotFoundError:
        print("âš ï¸ Model file not found. Skipping PPO evaluation.")

    # ==========================================
    # 5. å¼€å§‹æ¯”èµ› (Tournament)
    # ==========================================
    results = {}
    print("\n" + "="*40)
    print("ğŸ STARTING TOURNAMENT ğŸ")
    print("="*40 + "\n")
    
    for name, agent in agents.items():
        score = run_evaluation(name, agent, env, n_episodes=3)
        results[name] = score

    # 6. Final Report
    print("="*50)
    print("ğŸ“Š FINAL RESULTS (Lower CMO is Better)")
    print("="*50)
    
    # Sort by CMO (ascending is better)
    sorted_results = dict(sorted(results.items(), key=lambda item: item[1]))
    
    baseline = results.get("Greedy (Standard)", None)
    
    for name, score in sorted_results.items():
        diff_str = ""
        if baseline and name != "Greedy (Standard)":
            pct_change = ((score - baseline) / baseline) * 100
            if pct_change < 0:
                diff_str = f"(âœ… Improved by {abs(pct_change):.1f}%)"
            else:
                diff_str = f"(âŒ Worsened by {pct_change:.1f}%)"
        
        print(f"{name.ljust(20)}: {score:,.0f} CMO {diff_str}")

if __name__ == "__main__":
    main()